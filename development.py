# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EEJtSUjZypaZgB8fnkGjVfqPTZ7xM16U
"""

!pip install requests beautifulsoup4 transformers faiss-cpu

import requests
from bs4 import BeautifulSoup
from transformers import AutoTokenizer, AutoModel
import torch
import os

# Data extraction
url = "https://brainlox.com/courses/category/technical"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

courses = []
for course in soup.find_all('div', class_='single-courses-box'):
    title = course.find('h3').get_text(strip=True) if course.find('h3') else "N/A"
    description = course.find('p').get_text(strip=True) if course.find('p') else "N/A"
    price = course.find('span', class_='price-per-session').get_text(strip=True) if course.find('span', class_='price-per-session') else "N/A"
    lessons = course.find('li').get_text(strip=True) if course.find('li') else "N/A"
    link = course.find('a', href=True)['href'] if course.find('a', href=True) else "N/A"

    courses.append({
        "title": title,
        "description": description,
        "price": price,
        "lessons": lessons,
        "link": link
    })

# Check if courses were extracted
print(f"Found {len(courses)} courses.")
print(courses)

# Embeddings generation
model_name = 'sentence-transformers/all-MiniLM-L6-v2'  # Example model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def get_embeddings(texts):
    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).numpy()

texts = [course['description'] for course in courses]
course_embeddings = get_embeddings(texts)

# Save the vector store
from langchain.vectorstores import FAISS
from langchain.docstore.in_memory import InMemoryDocstore
import faiss

index = faiss.IndexFlatL2(course_embeddings.shape[1])
docstore = InMemoryDocstore({i: {"description": t} for i, t in enumerate(texts)})
index_to_docstore_id = {i: str(i) for i in range(len(texts))}

vector_store = FAISS(
    course_embeddings,
    index,
    docstore,
    index_to_docstore_id
)

vector_store.save_local("/content/path_to_store")